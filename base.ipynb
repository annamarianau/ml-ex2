{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# %run './base.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "- impute (various kinds)/dummy values/drop missing values\n",
    "- outlier removal?\n",
    "- Scale (standardize)/don't scale \n",
    "- feature selection (PCA, etc.)/none\n",
    "\n",
    "## Models\n",
    "- Linear Regression (Anna)\n",
    "- SVR (Moritz)\n",
    "- GradientBoostingRegressor (David)\n",
    "\n",
    "## Params\n",
    "- cv=10\n",
    "- scoring=['r2', 'neg_mean_absolute_error', 'neg_mean_squared_error']\n",
    "- Best params wählen nach MSE, aber auch r^2 notieren für Vergleichbarkeit\n",
    "\n",
    "## AutoML\n",
    "- \n",
    "\n",
    "\n",
    "## Datasets\n",
    "- Bike sharing (Kaggle) ()\n",
    "    - https://www.kaggle.com/c/184702-tu-ml-ws-18-bike-sharing#_=_\n",
    "    - large samples (train = 8690), small dimension (15)\n",
    "    - attribute characteristics: numeric, date?\n",
    "- Student performance (Kaggle) (Moritz)\n",
    "    - https://www.kaggle.com/c/184702-tu-ml-ws-18-student-performance\n",
    "    - small samples (train = 198), medium dimension (32)\n",
    "    - attribute characteristics: numeric, categorical \n",
    "- Blog feedback (David)\n",
    "    - https://archive.ics.uci.edu/ml/datasets/BlogFeedback\n",
    "    - very large samples (60021), large dimension (281)\n",
    "    - attribute characteristics: numeric\n",
    "- Forest fires (Anna)\n",
    "    - https://archive.ics.uci.edu/ml/datasets/Forest+Fires\n",
    "    - medium samples (513), small dimension (13) \n",
    "    - attribute characteristics: numeric, categorical\n",
    "    \n",
    "## Steps\n",
    "- Imports for all Datasets\n",
    "- functions for all Regressors\n",
    "\n",
    "## Notes\n",
    "On MSE being negative:\n",
    "\n",
    "```\n",
    "Yes, this is supposed to happen. The actual MSE is simply the positive version of the number you're getting.\n",
    "\n",
    "The unified scoring API always maximizes the score, so scores which need to be minimized are negated in order for the unified scoring API to work correctly. The score that is returned is therefore negated when it is a score that should be minimized and left positive if it is a score that should be maximized.\n",
    "```\n",
    "from https://stackoverflow.com/questions/21443865/scikit-learn-cross-validation-negative-values-with-mean-squared-error, \n",
    "\n",
    "also described in https://stackoverflow.com/questions/21050110/sklearn-gridsearchcv-with-pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annanau/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: OMP_NUM_THREADS=None =>\n",
      "... If you are using openblas if you are using openblas set OMP_NUM_THREADS=1 or risk subprocess calls hanging indefinitely\n"
     ]
    }
   ],
   "source": [
    "# Modules\n",
    "\n",
    "# import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import time\n",
    "from time import strptime\n",
    "from datetime import datetime\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "from matplotlib import cm as cm\n",
    " \n",
    "# general sklearn/preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# SVR\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Gradient Boosted Tree\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# AutoML\n",
    "from hpsklearn import HyperoptEstimator, svr_linear, gradient_boosting_regression, pca, normalizer, standard_scaler\n",
    "from hyperopt import hp\n",
    "from hyperopt import anneal, rand, tpe, mix\n",
    "\n",
    "# allows to output plots in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# scale data\n",
    "def scale_data(train_data, test_data = pd.DataFrame):\n",
    "    \"\"\"Standardize features by removing the mean and scaling to unit variance\n",
    "    \n",
    "    Only scales columns with dtype 'float64' and 'int64'.\"\"\"\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler()\n",
    "  \n",
    "    # drop excluded cols, select numeric cols from data and cast them to float64\n",
    "    train = train_data.select_dtypes(['float64', 'int64']).astype('float64')\n",
    "    test = test_data.select_dtypes(['float64', 'int64']).astype('float64')\n",
    "    \n",
    "    train_index, train_columns = train.index, train.columns\n",
    "    test_index, test_columns = test.index, test.columns\n",
    "        \n",
    "    # Fit on training set only.\n",
    "    scaler.fit(train)\n",
    "\n",
    "    # Apply transform to both the training set and the test set.\n",
    "    train_s = pd.DataFrame(scaler.transform(train[train_columns].values), index=train_index, columns=train_columns)\n",
    "    train_data.update(train_s)\n",
    "    if (test.empty):\n",
    "        return (train_data)\n",
    "    else:\n",
    "        test_s = pd.DataFrame(scaler.transform(test[test_columns].values), index=test_index, columns=test_columns)\n",
    "        test_data.update(test_s)\n",
    "        return (train_data, test_data)\n",
    "\n",
    "# replace empty strings with nan\n",
    "def fillspace_nan(data):\n",
    "    return data.apply(lambda x: x.replace('', np.nan))\n",
    "\n",
    "# strip whitespaces\n",
    "def strip(data):\n",
    "    return data.apply(lambda x: x.str.strip())\n",
    "\n",
    "# one hot encoding\n",
    "def one_hot(data, drop_first = True):\n",
    "    columns = data.select_dtypes(['object']).columns\n",
    "    return pd.get_dummies(data, columns = columns, drop_first = drop_first)\n",
    "\n",
    "# PCA\n",
    "def my_pca(train_data, test_data, n_comp):\n",
    "    pca = PCA(n_components = n_comp)\n",
    "    pca.fit(train_data)\n",
    "    pca_train = pd.DataFrame(pca.transform(train_data))\n",
    "    pca_test = pd.DataFrame(pca.transform(test_data))\n",
    "    return (pca_train, pca_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring\n",
    "def rmse(neg_mean_square_error):\n",
    "    return math.sqrt(-1.*neg_mean_square_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "def linear_reg(X_train, y_train, X_test=pd.DataFrame(), y_test=pd.DataFrame()):\n",
    "    # Fit model\n",
    "    reg = LinearRegression().fit(X_train, y_train)\n",
    "    print(f'R^2 value for model: {round(reg.score(X_train, y_train), 5)}')\n",
    "    #print(f'Coeffiecients: {reg.coef_}')\n",
    "    \n",
    "    # Predict test, compute metrics\n",
    "    if((not(X_test.empty)) & (not(y_test.empty))):\n",
    "        print(\"Predict:\")\n",
    "        pred = reg.predict(X_test)\n",
    "        r2 = round(reg.score(X_test, y_test), 5)\n",
    "        mse = round(math.sqrt(mean_squared_error(y_test, pred)), 5)\n",
    "        print(f'RMSE: {mse}')\n",
    "        print(f'R^2 Score: {r2}')\n",
    "    \n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Regression \n",
    "def run_svr(X_train, y_train, cv, param_grid, \n",
    "            X_test=pd.DataFrame(), y_test=pd.DataFrame()):\n",
    "    \"\"\"Run SVR\n",
    "    \n",
    "    Keyword arguments:\n",
    "    X_train -- train data\n",
    "    y_train -- train target\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"GridSearch initializing...\")\n",
    "    gs = GridSearchCV(estimator = SVR(), cv=cv, param_grid=param_grid, iid=True,\n",
    "                      scoring=['r2', 'neg_mean_absolute_error', 'neg_mean_squared_error'], \n",
    "                      refit='neg_mean_squared_error')\n",
    "    \n",
    "    print(\"SVR model in training...\")\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    print(f'''MSE: {round(-1.*gs.best_score_, 5)}, ''' \\\n",
    "          f'''RMSE: {round(rmse(gs.best_score_), 5)}, ''')\n",
    "    print(f'''C: {gs.best_estimator_.C}, ''' \\\n",
    "          f'''kernel: {gs.best_estimator_.kernel}, ''' \\\n",
    "          f'''epsilon: {gs.best_estimator_.epsilon}, ''' \\\n",
    "          f'''gamma: {gs.best_estimator_.gamma} ''')\n",
    "    \n",
    "    # Predict test, compute metrics\n",
    "    if((not(X_test.empty)) & (not(y_test.empty))):\n",
    "        print(\"Predict:\")\n",
    "        pred = gs.predict(X_test)\n",
    "        mse = round(math.sqrt(mean_squared_error(y_test, pred)), 5)\n",
    "        r2 = round(r2_score(y_test, pred), 5)\n",
    "        print(f'RMSE: {mse}')\n",
    "        print(f'R^2 Score: {r2}')\n",
    "\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "def run_boosted_tree(train_data, train_target, test_data, test_target, param_fix, cv, param_grid):\n",
    "    print(\"GridSearch initializing...\")\n",
    "    clf = GridSearchCV(estimator = GradientBoostingRegressor(**param_fix), cv = cv, param_grid = param_grid,  \n",
    "                       scoring = ['r2', 'neg_mean_absolute_error', 'neg_mean_squared_error'], \n",
    "                       refit = 'neg_mean_squared_error', iid=True)\n",
    "    \n",
    "    print(\"GradientBoostedRegressor model in training...\")\n",
    "    t0 = time.time()\n",
    "    clf.fit(train_data, train_target)\n",
    "    clf_fit = time.time() - t0\n",
    "    print(\"GradientBoostedRegressor model selected and fitted in %.3f s\\n\" % clf_fit)\n",
    "    \n",
    "    print(f'''MSE: {round(-1.*clf.best_score_, 5)}, ''' \\\n",
    "          f'''RMSE: {round(rmse(clf.best_score_), 5)}''')\n",
    "    \n",
    "    print(f'Best parameters selected by GridSearch: {clf.best_params_}')\n",
    "    \n",
    "    return clf\n",
    "\n",
    "# does not work with GridSearch\n",
    "def plot_training_deviance(clf, X_test, y_test):\n",
    "    test_score = np.zeros((clf.best_params_['n_estimators'],), dtype=np.float64)\n",
    "\n",
    "    for i, y_pred in enumerate(clf.staged_predict(X_test)):\n",
    "        test_score[i] = clf.loss_(y_test, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('Deviance')\n",
    "    plt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',\n",
    "             label='Training Set Deviance')\n",
    "    plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n",
    "             label='Test Set Deviance')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('Boosting Iterations')\n",
    "    plt.ylabel('Deviance')\n",
    "\n",
    "# kinda not works as expected\n",
    "def plot_scores(results):\n",
    "    scoring = ['r2', 'neg_mean_squared_error']\n",
    "    \n",
    "    plt.figure(figsize=(13, 13))\n",
    "    plt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",\n",
    "          fontsize=16)\n",
    "\n",
    "    plt.xlabel(\"n_estimators\")\n",
    "    plt.ylabel(\"Score\")\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim(0, 402)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    # Get the regular numpy array from the MaskedArray\n",
    "    X_axis = np.array(results['param_n_estimators'].data, dtype=float)\n",
    "\n",
    "    for scorer, color in zip(sorted(scoring), ['g', 'k']):\n",
    "        for sample, style in (('train', '--'), ('test', '-')):\n",
    "            sample_score_mean = results['mean_%s_%s' % (sample, scorer)]\n",
    "            sample_score_std = results['std_%s_%s' % (sample, scorer)]\n",
    "            ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n",
    "                            sample_score_mean + sample_score_std,\n",
    "                            alpha=0.1 if sample == 'test' else 0, color=color)\n",
    "            ax.plot(X_axis, sample_score_mean, style, color=color,\n",
    "                    alpha=1 if sample == 'test' else 0.7,\n",
    "                    label=\"%s (%s)\" % (scorer, sample))\n",
    "\n",
    "        best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n",
    "        best_score = results['mean_test_%s' % scorer][best_index]\n",
    "\n",
    "        # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "        ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n",
    "                linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n",
    "\n",
    "        # Annotate the best score for that scorer\n",
    "        ax.annotate(\"%0.2f\" % best_score,\n",
    "                    (X_axis[best_index], best_score + 0.005))\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoML\n",
    "\n",
    "# Make sure training_frame, X_test, y_test are H2OFrame dataframes \n",
    "def run_autoML2(y, training_frame, X_test, y_test):\n",
    "\n",
    "    # Run AutoML for 20 base models (limited to 1 hour max runtime by default)\n",
    "    aml = H2OAutoML(max_models=30, max_runtime_secs=300, seed=1)\n",
    "    aml.train(y=y, training_frame=training_frame)\n",
    "    \n",
    "    # View the AutoML Leaderboard\n",
    "    display('AutoML Leaderboard')\n",
    "    lb = aml.leaderboard\n",
    "    print(lb.head(rows=lb.nrows)) # Print all rows instead of default (10 rows)\n",
    "    \n",
    "    # Predict test set\n",
    "    preds = aml.predict(X_test)\n",
    "    preds_array = preds.as_data_frame().as_matrix() # First convert preds to a pandas df, then to a numpy array\n",
    "    rmse = round(math.sqrt(mean_squared_error(y_test, preds_array)), 5)\n",
    "    r2 = round(r2_score(y_test, preds_array), 5)\n",
    "    print(f'RMSE: {rmse}')\n",
    "    print(f'R^2 Score: {r2}')\n",
    "    \n",
    "    return aml\n",
    "\n",
    "def run_autoML(train_data, train_target, test_data, test_target, \n",
    "               preprocessing_ = {None}, classifier_ = {None}):\n",
    "    # define set of classifier\n",
    "    if classifier_ == {None}:\n",
    "        #classifier_ = hp.choice('clf',[GradientBoostingRegressor(learning_rate = 0.01, loss = 'ls')])\n",
    "        classifier_ = hp.choice('clf',[\n",
    "            svr_linear('svr', max_iter=1e5),\n",
    "            gradient_boosting_regression('gbt', learning_rate = 0.01, loss = 'ls')])\n",
    "    else:\n",
    "        classifier_ = classifier_\n",
    "        \n",
    "    # define preprocessing options\n",
    "    if preprocessing_ == {None}:\n",
    "        preprocessing_ = hp.choice('preproc',\n",
    "            [pca('pca'), normalizer('norm'),\n",
    "            standard_scaler('stand')])\n",
    "    else:\n",
    "        preprocessing_ = preprocessing_\n",
    "    \n",
    "    print(\"AutoML estimator initializing...\")\n",
    "    estim = HyperoptEstimator(\n",
    "        regressor=classifier_,\n",
    "        preprocessing=preprocessing_,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals = 30,\n",
    "        trial_timeout=300)\n",
    "    \n",
    "    print(\"AutoML estimator in training...\")\n",
    "    t0 = time.time()\n",
    "    estim.fit(train_data, train_target, cv_shuffle = False)\n",
    "    estim_fit = time.time() - t0\n",
    "    print(\"Best classifier selected and fitted in %.3f s\\n\" % estim_fit)\n",
    "    \n",
    "    print(\"Scoring: \" + estim.score(test_data, test_target))\n",
    "    print(\"Best selected model: \" + estim.best_model())\n",
    "    \n",
    "    \n",
    "def automl2(X_train, y_train, X_test, y_test):\n",
    "    print('Init AutoML')\n",
    "    est = HyperoptEstimator()\n",
    "    print('Fit AutoML')\n",
    "    est.fit(X_train, y_train)\n",
    "    model = est.best_model()\n",
    "    print(f'AutoML score: {est.score(X_test, y_test)}')\n",
    "    return est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "\n",
    "def boxplots(data):\n",
    "    for col in data.select_dtypes(['int64', 'float64']).columns:\n",
    "        fig, ax = plt.subplots()\n",
    "        bp = ax.boxplot(data[[col]].values)\n",
    "        ax.set_xlabel(col)\n",
    "        \n",
    "def histograms(data):\n",
    "    for col in data.select_dtypes(['object']).columns:\n",
    "        fig, ax = plt.subplots()\n",
    "        hg = ax.hist(data[[col]].values)\n",
    "        ax.set_xlabel(col)\n",
    "        \n",
    "def correlation_matrix(df, labels):\n",
    "    # sort by col names \n",
    "    r_df = df.reindex(labels, axis='columns')\n",
    "    \n",
    "    # create figure\n",
    "    fig = plt.figure(0, figsize=(20,20))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    # get colormap\n",
    "    cmap = cm.get_cmap('jet', 30)\n",
    "    # show correlation matrix on 2D raster\n",
    "    cax = ax1.imshow(r_df.corr(), interpolation=\"nearest\", cmap=cmap)\n",
    "    #ax1.grid(True)\n",
    "    \n",
    "    # add labels and ticks\n",
    "    plt.title('Correlation')\n",
    "    #ax1.set_xticks(range(r_df.shape[1]))\n",
    "    #ax1.set_xticklabels(labels, fontsize=10, rotation='vertical')\n",
    "    #ax1.set_yticks(range(r_df.shape[1]))\n",
    "    #ax1.set_yticklabels(labels, fontsize=10)\n",
    "    \n",
    "    # Add colorbar, make sure to specify tick locations to match desired ticklabels\n",
    "    fig.colorbar(cax, ticks=[-1, -.75, -.5, -.25, 0, .25, .5, .75, 1])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
