{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# %run './base.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "- impute (various kinds)/dummy values/drop missing values\n",
    "- outlier removal?\n",
    "- Scale (standardize)/don't scale \n",
    "- feature selection (PCA, etc.)/none\n",
    "\n",
    "## Models\n",
    "- Linear Regression (Anna)\n",
    "- SVR (Moritz)\n",
    "- GradientBoostingRegressor (David)\n",
    "\n",
    "## Params\n",
    "- cv=10\n",
    "- scoring=['r2', 'neg_mean_absolute_error', 'neg_mean_squared_error']\n",
    "- Best params wählen nach MSE, aber auch r^2 notieren für Vergleichbarkeit\n",
    "\n",
    "## AutoML\n",
    "- \n",
    "\n",
    "## Datasets\n",
    "- Bike sharing (Kaggle) ()\n",
    "    - https://www.kaggle.com/c/184702-tu-ml-ws-18-bike-sharing#_=_\n",
    "    - large samples (train = 8690), small dimension (15)\n",
    "    - attribute characteristics: numeric, date?\n",
    "- Student performance (Kaggle) (Moritz)\n",
    "    - https://www.kaggle.com/c/184702-tu-ml-ws-18-student-performance\n",
    "    - small samples (train = 198), medium dimension (32)\n",
    "    - attribute characteristics: numeric, categorical \n",
    "- Blog feedback (David)\n",
    "    - https://archive.ics.uci.edu/ml/datasets/BlogFeedback\n",
    "    - very large samples (60021), large dimension (281)\n",
    "    - attribute characteristics: numeric\n",
    "- Forest fires (Anna)\n",
    "    - https://archive.ics.uci.edu/ml/datasets/Forest+Fires\n",
    "    - medium samples (513), small dimension (13) \n",
    "    - attribute characteristics: numeric\n",
    "    \n",
    "## Steps\n",
    "- Imports for all Datasets\n",
    "- functions for all Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annanau/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# Modules\n",
    "\n",
    "# import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from time import strptime\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# allows to output plots in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# scale data\n",
    "def scale_data(train_data, test_data = pd.DataFrame):\n",
    "  scaler = preprocessing.StandardScaler()\n",
    "  \n",
    "  # Fit on training set only.\n",
    "  scaler.fit(train_data)\n",
    "  \n",
    "  # Apply transform to both the training set and the test set.\n",
    "  train_data[train_data.columns] = pd.DataFrame(scaler.transform(train_data[train_data.columns]))\n",
    "  if (test_data.empty):\n",
    "    return (train_data)\n",
    "  else:\n",
    "    test_data[test_data.columns] = pd.DataFrame(scaler.transform(test_data[test_data.columns]))\n",
    "    return (train_data, test_data)\n",
    "\n",
    "# replace empty strings with nan\n",
    "def fillspace_nan(data):\n",
    "  return data.apply(lambda x: x.replace('', np.nan))\n",
    "\n",
    "# strip whitespaces\n",
    "def strip(data):\n",
    "  return data.apply(lambda x: x.str.strip())\n",
    "\n",
    "# one hot encoding\n",
    "def one_hot(data, drop_first = True):\n",
    "  columns = data.select_dtypes(['object'])\n",
    "  return pd.get_dummies(data, columns = columns, drop_first = True)\n",
    "\n",
    "# PCA\n",
    "def pca(train_data, test_data, n_comp):\n",
    "  pca = PCA(n_components = n_comp)\n",
    "  pca.fit(train_data)\n",
    "  pca_train = pd.DataFrame(pca.transform(train_data))\n",
    "  pca_test = pd.DataFrame(pca.transform(test_data))\n",
    "  return (pca_train, pca_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "def linear_reg(X_train, y_train, X_test, y_test, get_coef = True):\n",
    "    # Build model\n",
    "    reg = LinearRegression().fit(X_train, y_train)\n",
    "    r2_score = reg.score(X_train, y_train)\n",
    "    \n",
    "    # Predict test data and compute MSE\n",
    "    y_pred = reg.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    metrics = {'R2 Score: ': [r2_score], 'MSE: ' : mse}\n",
    "    \n",
    "    coef = {}\n",
    "    if get_coef:\n",
    "        coef = {\n",
    "            'Coefficients: ' :reg.coef_\n",
    "        }\n",
    "    return metrics, coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "def run_boosted_tree(train_data, train_target, test_data, test_target, param_fix, cv, param_grid):\n",
    "    print(\"GridSearch initializing...\")\n",
    "    clf = GridSearchCV(estimator = GradientBoostingRegressor(**param_fix), cv = cv, param_grid = param_grid, \n",
    "                       scoring = ['r2', 'neg_mean_absolute_error', 'neg_mean_squared_error'], \n",
    "                       refit = 'neg_mean_squared_error')\n",
    "    \n",
    "    print(\"GradientBoostedRegressor model in training...\")\n",
    "    t0 = time.time()\n",
    "    clf.fit(train_data, train_target)\n",
    "    clf_fit = time.time() - t0\n",
    "    print(\"GradientBoostedRegressor model selected and fitted in %.3f s\\n\" % clf_fit)\n",
    "    \n",
    "    best_params = clf.best_params_\n",
    "    print(\"Best parameters selected by GridSearch: %s\" % best_params)\n",
    "    \n",
    "    return clf\n",
    "\n",
    "# does not work with GridSearch!!!\n",
    "def plot_training_deviance(clf, X_test, y_test):\n",
    "    test_score = np.zeros((clf.best_params_['n_estimators'],), dtype=np.float64)\n",
    "\n",
    "    for i, y_pred in enumerate(clf.staged_predict(X_test)):\n",
    "        test_score[i] = clf.loss_(y_test, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('Deviance')\n",
    "    plt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',\n",
    "             label='Training Set Deviance')\n",
    "    plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n",
    "             label='Test Set Deviance')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('Boosting Iterations')\n",
    "    plt.ylabel('Deviance')\n",
    "\n",
    "# kinda not works as expected\n",
    "def plot_scores(results):\n",
    "    scoring = ['r2', 'neg_mean_squared_error']\n",
    "    \n",
    "    plt.figure(figsize=(13, 13))\n",
    "    plt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",\n",
    "          fontsize=16)\n",
    "\n",
    "    plt.xlabel(\"n_estimators\")\n",
    "    plt.ylabel(\"Score\")\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim(0, 402)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    # Get the regular numpy array from the MaskedArray\n",
    "    X_axis = np.array(results['param_n_estimators'].data, dtype=float)\n",
    "\n",
    "    for scorer, color in zip(sorted(scoring), ['g', 'k']):\n",
    "        for sample, style in (('train', '--'), ('test', '-')):\n",
    "            sample_score_mean = results['mean_%s_%s' % (sample, scorer)]\n",
    "            sample_score_std = results['std_%s_%s' % (sample, scorer)]\n",
    "            ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n",
    "                            sample_score_mean + sample_score_std,\n",
    "                            alpha=0.1 if sample == 'test' else 0, color=color)\n",
    "            ax.plot(X_axis, sample_score_mean, style, color=color,\n",
    "                    alpha=1 if sample == 'test' else 0.7,\n",
    "                    label=\"%s (%s)\" % (scorer, sample))\n",
    "\n",
    "        best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n",
    "        best_score = results['mean_test_%s' % scorer][best_index]\n",
    "\n",
    "        # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "        ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n",
    "                linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n",
    "\n",
    "        # Annotate the best score for that scorer\n",
    "        ax.annotate(\"%0.2f\" % best_score,\n",
    "                    (X_axis[best_index], best_score + 0.005))\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
