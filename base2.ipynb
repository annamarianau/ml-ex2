{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "- One simple feature representation (color histogram)\n",
    "- One more advanced feature extractor based on SIFT\n",
    "- https://en.wikipedia.org/wiki/Scale-invariant_feature_transform\n",
    "    - and subseqent visual bag of words\n",
    "    - https://kushalvyas.github.io/BOV.html\n",
    "\n",
    "\n",
    "- try a few algorithms\n",
    "- especially MLPs\n",
    "- try different parameter settings\n",
    "- -> this is the baseline\n",
    "\n",
    "\n",
    "- then try a deep convolutional neural network\n",
    "    - Tensorflow\n",
    "    \n",
    "    \n",
    "- detailed comparison and analysis\n",
    "    - confusion matrix\n",
    "    - other patterns\n",
    "- log everything\n",
    "\n",
    "## TODO\n",
    "- relative path needed for standalone\n",
    "- MLP tuning\n",
    "- find example for CNN (check how many input nodes we need, number of layers)\n",
    "\n",
    "\n",
    "    -> understand and create CNN\n",
    "    -> improve KNN and MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# set pandas to show more columns\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 800)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn import neighbors\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#from matplotlib import pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "def run_cv(classifier, train_data, train_target, num_cv = 10):\n",
    "    scores_acc = model_selection.cross_val_score(classifier, train_data, train_target, cv = num_cv)\n",
    "    scores_f1 = model_selection.cross_val_score(classifier, train_data, train_target, cv = num_cv, scoring = 'f1_macro', n_jobs = -1)\n",
    "    print(f\"Accuracy: {scores_acc.mean():.3f} (+/- {scores_acc.std():.3f}), F1: {scores_f1.mean():.3f} (+/- {scores_f1.std():.3f})\")\n",
    "\n",
    "# Print confusion matrix\n",
    "def conf_matrix(true, pred, classes):\n",
    "    display(pd.DataFrame(metrics.confusion_matrix(true, pred), index=classes, columns=classes))\n",
    "    \n",
    "# Print result comparison\n",
    "def comp_labels(true, pred, num=None):\n",
    "    if (num == None): num = len(true)\n",
    "    display(pd.DataFrame([pd.Series(true[:num]), pd.Series(pred[:num])], index=['true', 'pred']).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use for packaged python file?\n",
    "#os.path.dirname(os.path.abspath(__file__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import fruit dataset\n",
    "def get_fruit_labels():\n",
    "    \"\"\"Get folder + file names from fruits dataset. Must be called when os.getcwd is the project folder.\"\"\"\n",
    "    print(f\"Getting fruit labels - {datetime.datetime.now()}\")\n",
    "    file_names = glob.glob('FIDS30/*/*.jpg')\n",
    "    print(f\"Found {len(file_names)} files\\n\")\n",
    "\n",
    "    # get target labels from file path\n",
    "    file_names_trunc = []\n",
    "    target_labels = []\n",
    "    for file_name in file_names:\n",
    "        #if (user == 'david'): \n",
    "        file_name_trunc = file_name[7:]\n",
    "        file_names_trunc.append(file_name_trunc)\n",
    "        try:\n",
    "            pathSepIndex = file_name_trunc.index(\"\\\\\") # david\n",
    "        except: \n",
    "            pathSepIndex = file_name_trunc.index(\"/\")\n",
    "        target_labels.append(file_name_trunc[:pathSepIndex])\n",
    "    print(f\"Done getting labels - {datetime.datetime.now()}\\n\")\n",
    "    return file_names_trunc, target_labels\n",
    "\n",
    "def int_encode_labels(target_labels):\n",
    "    # preprocessing needed as sklearn can only work with integer labels\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    label_encoder.fit(target_labels) # find all unique class names, assign them to the numbers\n",
    "    target = label_encoder.transform(target_labels)\n",
    "    print(f\"Classes transformed: {label_encoder.classes_}\\n\")\n",
    "    return target, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_car_labels():\n",
    "    print(f\"Getting car labels - {datetime.datetime.now()}\")\n",
    "    path = 'CarData/TrainImages'\n",
    "    files = glob.glob(os.path.join(path, '*.pgm'))\n",
    "    print(\"Found\", len(files), \"train files\")\n",
    "    \n",
    "    images = []\n",
    "    image_names = []\n",
    "\n",
    "    for filename in files:\n",
    "        image_names.append(os.path.basename(filename))\n",
    "        with Image.open(filename) as img:\n",
    "            images.append(np.array(img)) # we convert the images to a Numpy array and store them in a list\n",
    "            \n",
    "    classes = []\n",
    "    for name in image_names:\n",
    "        if name.startswith('neg'):\n",
    "            classes.append(0)\n",
    "        else:\n",
    "            classes.append(1)\n",
    "            \n",
    "    test_path = 'CarData/TestImages'\n",
    "    test_files = glob.glob(os.path.join(test_path, '*.pgm'))\n",
    "    print(\"Found\", len(test_files), \"test files\")\n",
    "    \n",
    "    test_images = []\n",
    "    test_image_names = []\n",
    "\n",
    "    for filename in test_files:\n",
    "        test_image_names.append(os.path.basename(filename))\n",
    "        with Image.open(filename) as img:\n",
    "            img_resized = resize_and_crop(img,target_width=100,target_height=40)\n",
    "            test_images.append(np.array(img_resized))\n",
    "            \n",
    "    test_classes = [1] * len(test_files)\n",
    "    print(f\"Done getting labels - {datetime.datetime.now()}\\n\")\n",
    "    return image_names, classes, test_image_names, test_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pil_extraction(images_path, file_names):\n",
    "    \"\"\"Extracts histogram features using PIL/PILLOW\"\"\"\n",
    "    print(f\"Starting PIL extraction - {datetime.datetime.now()}\")\n",
    "\n",
    "    # The simplest approach is via the PIL/PILLOW package; here we get a histogram over each RGB channel\n",
    "    # Note: this doesn't really represent colours, as a colour is made up of the combination of the three channels!\n",
    "    train_histo = []\n",
    "    for index, file_name in enumerate(file_names):\n",
    "        image_PIL = Image.open(images_path + file_name)\n",
    "        image_PIL = image_PIL.convert('RGB')     \n",
    "        feature_vector = image_PIL.histogram()\n",
    "\n",
    "        if (len(feature_vector) != 768): # just a sanity check; with the transformation to RGB, this should never happen\n",
    "            print(\"Unexpected length of feature vector: \" + str(len(feature_vector)) + \" in file: \" + file_name)\n",
    "\n",
    "        train_histo.append((feature_vector))\n",
    "    print(f\"Done - {datetime.datetime.now()}\")\n",
    "    return train_histo\n",
    "\n",
    "def opencv_extraction(image_path, file_names):\n",
    "    ## additional feature extraction (optional)\n",
    "    # extracting more features / combinations\n",
    "\n",
    "    print(f\"Starting OpenCV extraction - {datetime.datetime.now()}\")\n",
    "\n",
    "    data_opencv_1D, data_opencv_2D, data_opencv_3D = [], [], []\n",
    "\n",
    "    # use our own simple function to flatten the 2D arrays\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "    for file_name in file_names:\n",
    "\n",
    "        # the easiest way would be to do the following:\n",
    "        # imageOpenCV = cv2.imread(imagePath + fileName)\n",
    "\n",
    "        # However, we have the same issue as before, and it is more difficult in OpenCV to convert to an RGB image\n",
    "        # Thus we do this using PIL, and then convert to OpenCV ....\n",
    "        image_PIL = Image.open(image_path + file_name)\n",
    "        image_PIL = image_PIL.convert('RGB')\n",
    "        image_opencv = np.array(image_PIL) \n",
    "        # Convert RGB to BGR \n",
    "        image_opencv = image_opencv[:, :, ::-1].copy() \n",
    "\n",
    "        # Now we split the image in the three channels, B / G / R\n",
    "        channels = cv2.split(image_opencv)\n",
    "        colors = (\"b\", \"g\", \"r\")\n",
    "\n",
    "        # First we do also features per channel, but this time, we aggregate them into a smaller number of bins\n",
    "        # I.e. we do not have 256 values per channel, but less\n",
    "        features_opencv_1D = []\n",
    "        bins_1D = 64\n",
    "        for (channel, color) in zip(channels, colors): # we compute the histogram over each channel\n",
    "            hist_opencv = cv2.calcHist([channel], [0], None, [bins_1D], [0, 256])\n",
    "            features_opencv_1D.extend(hist_opencv)\n",
    "        feature_vector_opencv_1D = flatten(features_opencv_1D) # and append this to our feature vector\n",
    "\n",
    "        data_opencv_1D.append(feature_vector_opencv_1D) # now we append the feature vector to the dataset so far\n",
    "\n",
    "        if (len(feature_vector_opencv_1D) != bins_1D*3): # sanity check, in case we had a wrong number of channels...\n",
    "            print(f\"Unexpected length of feature vector: {str(len(feature_vector_opencv_1D))} in file: {file_name}\")\n",
    "\n",
    "        # Next - features that look at two channels at the same time\n",
    "        # E.g. we look at when green and blue have both \"high values\"\n",
    "        # We reduce the size of bins further, to not have a too long feature vector\n",
    "        features_opencv_2D = []\n",
    "        bins_2D = 16\n",
    "        # look at all combinations of channels (R & B, R & G, B & G)\n",
    "        features_opencv_2D.extend(cv2.calcHist([channels[1], channels[0]], [0, 1], None, [bins_2D, bins_2D], [0, 256, 0, 256]))\n",
    "        features_opencv_2D.extend(cv2.calcHist([channels[1], channels[2]], [0, 1], None, [bins_2D, bins_2D], [0, 256, 0, 256]))\n",
    "        features_opencv_2D.extend(cv2.calcHist([channels[0], channels[2]], [0, 1], None, [bins_2D, bins_2D], [0, 256, 0, 256]))\n",
    "        # and add that to our dataset\n",
    "        feature_vector_opencv_2D = flatten(features_opencv_2D)\n",
    "        data_opencv_2D.append(feature_vector_opencv_2D)\n",
    "\n",
    "        # finally, we look at all three channels at the same time.\n",
    "        # We further reduce our bin size, because otherwise, this would become very large...\n",
    "        features_opencv_3D = cv2.calcHist([image_opencv], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "        # append to our dataset\n",
    "        feature_vector_opencv_3D = features_opencv_3D.flatten()\n",
    "        data_opencv_3D.append(feature_vector_opencv_3D)\n",
    "\n",
    "    print(f\"Done - {datetime.datetime.now()}\")\n",
    "    return data_opencv_1D, data_opencv_2D, data_opencv_3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SIFT feature extraction\n",
    "\n",
    "def sift_extraction(image_path, file_names):\n",
    "    print(f\"Starting SIFT extraction - {datetime.datetime.now()}\")\n",
    "\n",
    "    sift_kp = []\n",
    "    sift_desc = []\n",
    "    for file_name in file_names:\n",
    "        path = image_path + file_name\n",
    "        img = cv2.imread(path)\n",
    "        # Check if img has been read successfully\n",
    "        if (not img is None):\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            sift = cv2.xfeatures2d.SIFT_create()\n",
    "            kp, desc = sift.detectAndCompute(gray, None)\n",
    "            sift_kp.append(kp)\n",
    "            if (not desc is None):\n",
    "                sift_desc.append(desc)\n",
    "            else:\n",
    "                arr = np.empty((1, 128))\n",
    "                sift_desc.append(arr)\n",
    "        else: \n",
    "            print(image_path + file_name)\n",
    "\n",
    "    # cluster features into visual bag of words\n",
    "    K = 8 # default value in sklearn\n",
    "    cluster_model = MiniBatchKMeans(n_clusters=K)\n",
    "    n_clusters = cluster_model.n_clusters\n",
    "\n",
    "    # Concatenate all descriptors in the training set together\n",
    "    # training_descs = [sift_desc[i] for i in training_idxs]\n",
    "    training_descs = sift_desc\n",
    "    all_train_descriptors = [desc for desc_list in training_descs for desc in desc_list]\n",
    "    all_train_descriptors = np.array(all_train_descriptors)\n",
    "\n",
    "    if all_train_descriptors.shape[1] != 128:\n",
    "        raise ValueError('Expected SIFT descriptors to have 128 features, got', all_train_descriptors.shape[1])\n",
    "\n",
    "    print('%i descriptors before clustering' % all_train_descriptors.shape[0])\n",
    "\n",
    "    # Cluster descriptors to get codebook\n",
    "    print('Using clustering model %s...' % repr(cluster_model))\n",
    "    print('Clustering on training set to get codebook of %i words' % n_clusters)\n",
    "\n",
    "    # train kmeans or other cluster model on those descriptors selected above\n",
    "    cluster_model.fit(all_train_descriptors)\n",
    "    print('Done clustering. Using clustering model to generate BoW histograms for each image.')\n",
    "\n",
    "    # compute set of cluster-reduced words for each image\n",
    "    img_clustered_words = [cluster_model.predict(raw_words) for raw_words in sift_desc]\n",
    "\n",
    "    # finally make a histogram of clustered word counts for each image. These are the final features.\n",
    "    img_bow_hist = np.array(\n",
    "        [np.bincount(clustered_words, minlength=n_clusters) for clustered_words in img_clustered_words])\n",
    "\n",
    "    print('Done generating BoW histograms.')\n",
    "    print(f\"Done - {datetime.datetime.now()}\")\n",
    "    return img_bow_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(X, y):\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def run_knn(X_train, X_test, y_train, y_test, classes, k = 11):\n",
    "    print(f\"Run KNN - {datetime.datetime.now()}\\n\")\n",
    "    # define classifier\n",
    "    k = 11\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors = k)\n",
    "    # cross validation\n",
    "    run_cv(knn, X_train, y_train)\n",
    "\n",
    "    knn.fit(X_train, y_train)\n",
    "    knnresult = knn.predict(X_test)\n",
    "    \n",
    "    acc = metrics.accuracy_score(y_test, knnresult)\n",
    "    print(f\"[Test data] Accuracy: {acc:.3f}\")\n",
    "\n",
    "    # compare results\n",
    "    #comp_labels(y_test, knnresult, num=10)\n",
    "    #conf_matrix(y_test, knnresult, classes)\n",
    "    print(f\"Done - {datetime.datetime.now()}\\n\")\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_mlp(X_train, X_test, y_train, y_test, classes,\n",
    "            solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(3, 2), random_state=1):\n",
    "    print(f\"Run MLP - {datetime.datetime.now()}\\n\")\n",
    "    # MLP\n",
    "    clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(3, 2), random_state=1)\n",
    "    run_cv(clf, X_train, y_train)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    clfresult = clf.predict(X_test)\n",
    "    \n",
    "    acc = metrics.accuracy_score(y_test, clfresult)\n",
    "    print(f\"[Test data] Accuracy: {acc:.3f}\")\n",
    "\n",
    "    # compare results\n",
    "    #comp_labels(y_test, clfresult, num=10)\n",
    "    #conf_matrix(y_test, clfresult, classes)\n",
    "    print(f\"Done - {datetime.datetime.now()}\\n\")\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fruit Data\n",
    "fileNames, fruit_labels = get_fruit_labels()\n",
    "fruit_labels, labelEncoder = int_encode_labels(fruit_labels)\n",
    "print(f\"Test inverse transform: 0, 1, 18 => {list(labelEncoder.inverse_transform([0, 1, 18]))}\\n\")\n",
    "\n",
    "train_histo = pil_extraction('FIDS30/', fileNames)\n",
    "data_opencv_1D, data_opencv_2D, data_opencv_3D = opencv_extraction('FIDS30/', fileNames)\n",
    "\n",
    "train_bow = sift_extraction('FIDS30/', fileNames)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_histo, fruit_labels)\n",
    "\n",
    "knn_fruits = run_knn(X_train, X_test, y_train, y_test, labelEncoder.classes_)\n",
    "\n",
    "mlp_fruits = run_mlp(X_train, X_test, y_train, y_test, labelEncoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Car Data\n",
    "car_images_train, car_labels_train, car_images_test, car_labels_test = get_car_labels()\n",
    "\n",
    "car_histo_train = pil_extraction('CarData/TrainImages/', car_images_train)\n",
    "car_histo_test = pil_extraction('CarData/TestImages/', car_images_test)\n",
    "\n",
    "car_data_opencv_1D_train, car_data_opencv_2D_train, car_data_opencv_3D_train = opencv_extraction('CarData/TrainImages/', car_images_train)\n",
    "car_data_opencv_1D_test, car_data_opencv_2D_test, car_data_opencv_3D_test = opencv_extraction('CarData/TestImages/', car_images_test)\n",
    "\n",
    "car_bow_train = sift_extraction('CarData/TrainImages/', car_images_train)\n",
    "car_bow_test = sift_extraction('CarData/TestImages/', car_images_test)\n",
    "\n",
    "knn_car = run_knn(car_histo_train, car_histo_test, car_labels_train, car_labels_test, [0,1])\n",
    "mlp_car = run_mlp(car_histo_train, car_histo_test, car_labels_train, car_labels_test, [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_crop(img,target_width=224,target_height=224):\n",
    "    width, height = img.size\n",
    "\n",
    "    img_ratio = width / float(height)\n",
    "    target_ratio = target_width / float(target_height)\n",
    "\n",
    "    # 1) compare ratios and resize the larger side proportional to the target of the smaller side\n",
    "    new_width, new_height = (target_width, target_height)\n",
    "\n",
    "    if target_ratio > img_ratio:\n",
    "        new_height = int(round(height * (target_width / float(width))))\n",
    "    else:\n",
    "        new_width = int(round(width * (target_height / float(height))))\n",
    "\n",
    "    img_new = img.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "    # The filter argument can be one of NEAREST (use nearest neighbour), BILINEAR (linear interpolation in a\n",
    "    # 2x2 environment), BICUBIC (cubic spline interpolation in a 4x4 environment), or ANTIALIAS (a high-quality downsampling filter).\n",
    "    # If omitted, or if the image has mode \"1\" or \"P\", it is set to NEAREST.\n",
    "    # Note that the bilinear and bicubic filters in the current version of PIL are not well-suited for large downsampling\n",
    "    # ratios (e.g. when creating thumbnails). You should use ANTIALIAS unless speed is much more important than quality.\n",
    "\n",
    "    # 2) crop to target size\n",
    "    # offset to half of the remaining padding (one of them is 0)\n",
    "    width_offset  = (new_width - target_width) / 2\n",
    "    height_offset = (new_height - target_height) / 2\n",
    "\n",
    "    # crop with offsets\n",
    "    img_new = img_new.crop((width_offset, height_offset, width_offset+target_width, height_offset+target_height))\n",
    "    #  The box is a 4-tuple defining the left, upper, right, and lower pixel coordinate.\n",
    "    return img_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
